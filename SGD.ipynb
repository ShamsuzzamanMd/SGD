{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "Gradient descent (GD) is iterative optimization algorithm that minimizes the cost function to update the parameter in machine learning/deep learning. The basic idea in GD neural network is to pass the training set through the neural network and then update the parameters of the layers by computing the gradients. Depending upon how the training data is used to update the parameters of network, GD algorithm can be divided into different variants.\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "Let's start with stochastic gradient descent. In this method,d one training sample is passed through the neural network at a time and the parameters (weights) of each layer are updated with the computed gradient. For example, if the training set contains 100 samples then the parameters are updated 100 times that is one time after every individual example is passed through the network. Number of iterations (n): The number of times the gradient is estimated and the parameters of the neural network are updated using a batch of training instances. The batch size B is the number of training instances used in one iteration. In GD, one epoch is completed when all the training sampels are used only once. And number of times needed to complete one epoch is called iteration and number of training instances used in one iteration is called batch size. So, in stochastic gradient descent, we use the batch size of 1.\n",
    "\n",
    "# Batch Gradient Descent\n",
    "In retrospective, in Gradient descent also called as batch gradient descent, the parameters of network are updated only after all the training data have been passed through network. For example, if the training dataset contains 100 training examples then the parameters of the neural network are updated only when all 100 training instances are passed through network.\n",
    "\n",
    "# Mini batch Gradient Descent\n",
    "Mini batch gradient is compromize between stochastic and batch gradient descent. In batch gradient descent where parameter are updated only after all traning sampls are used , in stochastic gd, parameter are updated after each training sample, in mini batch gradient descent, dataset is divide into Number of Batches or sets or parts, with certain batch size( i.e number of samples in each batch) and parameter are updated after each batch. For example, we can divide the dataset of 2000 examples into batches of 500 (i.e batch size) and update parameter after training each batch. By this then it will take 4 iterations (4 batches) to complete 1 epoch.\n",
    "\n",
    "# Comparision of algorithms in terms of speed and convergence\n",
    "All these methods have their own advantages and disadvantages. Stochastic gradient descent it is easier to fit into memory due to a single training sample being processed by the network, it is computationally fast as only one sample is processed at a time, and for larger datasets it can converge faster as it causes updates to the parameters more frequently. However, it has also disadvantages. Due to frequent updates the steps taken towards the minima are very noisy. This can often lead the gradient descent into other directions. Also, due to noisy steps it may take longer to achieve convergence to the minima of the loss function.Frequent updates are computationally expensive due to using all resources for processing one training sample at a time. And batch gradient descent produces a more stable gradient descent convergence and stable error gradient than stochastic gradient descent.It is computationally efficient as all computer resources are not being used to process a single sample rather are being used for all training samples. It's disadvantages are that sometimes a stable error gradient can lead to a local minima and unlike stochastic gradient descent no noisy steps are there to help get out of the local minima. The entire training set can be too large to process in the memory due to which additional memory might be needed. Mini batch gradient descent easily fits in the memory, it is computationally efficient, benefit from vectorization. If stuck in local minimums, some noisy steps can lead the way out of them and at the same time average of the training samples produces stable error gradients and convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
